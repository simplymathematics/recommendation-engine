---
title: "Algorithmic Bias"
author: "simplymathematics"
date: "6/29/2019"
output: html_document
---


# Algorithmic Bias

It is well-documented that machine learning is both racist and sexist -- if only on accident. [Amazon's facial recognition platforms](https://time.com/5520558/artificial-intelligence-racial-gender-bias/) believes Oprah Winfrey (and many other black women) to be men. Search algorithms perpetuate racial biases that leads to results like, when [one searches for two different phrases online](https://www.washingtonpost.com/gdpr-consent/?destination=%2fopinions%2f2018%2f12%2f17%2fwhy-your-ai-might-be-racist%2f%3f&utm_term=.7b7d1505f6e3):

> Try typing “Asian girls” into the Google search bar. If your results are like mine, you’ll see a selection of come-ons to view sexy pictures, advice for “white men” on how to pick up Asian girls and some items unprintable in this publication. Try “Caucasian girls” and you’ll get wonky definitions of “Caucasian,” pointers to stock photos of wholesome women and kids, and some anodyne dating advice

This trend continues when searching for 'black girls,' a term that brings up sexually explicit images, blogs, and videos. 

This type of bias is also not new: 

> Algorithmic bias isn’t new. In the 1970s and 1980s, St. George’s Hospital Medical School in the United Kingdom used a computer program to do initial screening of applicants. The program, which mimicked the choices admission staff had made in the past, denied interviews to as many as 60 applicants because they were women or had non-European sounding names. 
[Source](https://muse.jhu.edu/article/645268/pdf)

[Amazon had a Recruitment Tool](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G) that showed bias against women

[Facebook is currently being sued](https://www.politico.com/story/2019/03/28/hud-charges-facebook-with-housing-discrimination-1241406) by the Department of Housing and Urban development because of methods and practices that lead to housing discrimination, not on purpose, of course.


[In another article](https://www.wired.com/story/the-real-reason-tech-struggles-with-algorithmic-bias/) written by a former intelligence office, he describes is his time at Facebook as a cognitive bias researcher and says

> These are mistakes made while trying to do the right thing. But they demonstrate why tasking untrained engineers and data scientists with correcting bias is, at the broader level, naïve, and at a leadership level insincere.

> I believe that many of my former coworkers at Facebook fundamentally want to make the world a better place. I have no doubt that they feel they are building products that have been tested and analyzed to ensure they are not perpetuating the nastiest biases. But the company has created its own sort of insular bubble in which its employees' perception of the world is the product of a number of biases that are engrained within the Silicon Valley tech and innovation scene.

The author goes on to say:

> This is exactly why the tech industry needs to actually invest in real cognitive bias training and empower true experts to address these issues, as opposed to spouting platitudes. Countering bias takes work. While I don’t expect companies to put their employees through the same rigorous training of intelligence analysts, raising awareness of their cognitive limitations through workshops and training would be one concrete step.

If we don't take positive, active steps to combat this human tendency, it will translate any product that we try to build. [The Brookings Institute](https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/) catalogued instance of Algorithmic Bias

1. Bias in word associations:
> They found that European names were perceived as more pleasant than those of African-Americans, and that the words “woman” and “girl” were more likely to be associated with the arts instead of science and math, which were most likely connected to males. In analyzing these word-associations in the training data, the machine learning algorithm picked up on existing racial and gender biases shown by humans. 

2. Bias in online ads
> Latanya Sweeney, Harvard researcher and former chief technology officer at the Federal Trade Commission (FTC), found that online search queries for African-American names were more likely to return ads to that person from a service that renders arrest records, as compared to the ad results for white names.

3. Bias in criminal justice algorithms


And finally, which is probably most relevant to autonomous vehicles:

4. Bias in facial recognition technology 

> MIT researcher Joy Buolamwini found that the algorithms powering three commercially available facial recognition software systems were failing to recognize darker-skinned complexions. Generally, most facial recognition training data sets are estimated to be more than 75 percent male and more than 80 percent white. When the person in the photo was a white man, the software was accurate 99 percent of the time at identifying the person as male. According to Buolamwini’s research, the product error rates for the three products were less than one percent overall, but increased to more than 20 percent in one product and 34 percent in the other two in the identification of darker-skinned women as female.


[Here is](https://arxiv.org/pdf/1902.11097.pdf) a study from Georgia Tech about 5 other ways racial bias seeps into facial regonition systems.

The same Brookings Institute link says that: 

> The European Union recently released “Ethics Guidelines for Trustworthy AI,” which delineates seven governance principles: (1) human agency and oversight, (2) technical robustness and safety, (3) privacy and data governance, (4) transparency, (5) diversity, nondiscrimination and fairness, (6) environmental and societal well-being, and (7) accountability. The EU’s ethical framework reflects a clear consensus that it is unethical to “unfairly discriminate.” Within these guidelines, member states link diversity and nondiscrimination with principles of fairness, enabling inclusion and diversity throughout the entire AI system’s lifecycle. Their principles interpret fairness through the lenses of equal access, inclusive design processes, and equal treatment.

It's good that at least some guidelines exist.

Even though Jon Kleinberg from Cornell University pointed out “[a]n algorithm has no choice but to be premeditated,” there are ways to combat these tendencies. Brookings (again) says that

>  In the mitigation of bias and the management of the risks associated with the algorithm, collaborative work teams can compensate for the blind-spots often missed in smaller, segmented conversations and reviews. Bringing together experts from various departments, disciplines, and sectors will help facilitate accountability standards and strategies for mitigating online biases, including from engineering, legal, marketing, strategy, and communications.

Here we're talking about all of the algorithms involved in a company-- from production design, to execution, and the very employees that make up the company. 
It's wrong to assert that this problem is one that can be solved by computers. This bias is ours to own and ours to address.

[The Univesity of California San Francisco](https://diversity.ucsf.edu/resources/strategies-address-unconscious-bias) says that: 

1. Promote self-awareness and regonize the biases we all have
2. Since categorization gives rise to implicit biases naturally in huamns, it is important to aprroach these problems in a an open way
3. Exposure to people of different backgrounds is important, but
4. Facilitated discussions and training sessions on bias literacy had the greatest effect.

[In one study](https://psycnet.apa.org/record/2012-23868-001), researchers found that inducing empathy toward an Asian American charater reduced explicit prejudice towards Asian Americans.

> Results showed that participants in the control condition exhibited ingroup bias, displaying faster response times for judging good words after being exposed to “us” and faster response times for bad words after being exposed to “them.” Participants in the empathy viewing condition, however, did not show this bias. These results suggest that inducing empathy is a particularly robust form of prejudice reduction, one that not only improves explicit intergroup attitudes but also implicit attitudes.

What's clear is that 1) emotions are real and human. 2) Bias is the result of natural neurological processes 3) Implicit bias seeps into our technology and causes harm 4) This problem can be solved by exposure to diverse groups, facilitated discussions and trainings on bias literacy, and acting with empathy.





